{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:48:10.947681Z",
     "start_time": "2021-10-16T11:48:10.944686Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "#                                   untar=True, cache_dir='.',\n",
    "#                                   cache_subdir='')\n",
    "\n",
    "# dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "# os.listdir(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:49:09.525587Z",
     "start_time": "2021-10-16T11:49:09.501021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'imdbEr.txt', 'imdb.vocab', 'README']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"/home/sl/workspace/data/nlp/aclImdb\"\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:49:21.610548Z",
     "start_time": "2021-10-16T11:49:21.602366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unsup',\n",
       " 'neg',\n",
       " 'labeledBow.feat',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'urls_unsup.txt',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:50:10.563900Z",
     "start_time": "2021-10-16T11:50:09.716385Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:51:43.178383Z",
     "start_time": "2021-10-16T11:51:42.036725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seed = 123\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    f'{dataset_dir}/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    f'{dataset_dir}/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:52:36.396917Z",
     "start_time": "2021-10-16T11:52:36.323087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b\"I have watched this movie well over 100-200 times, and I love it each and every time I watched it. Yes, it can be very corny but it is also very funny and enjoyable. The camp shown in the movie is a real camp that I actually attended for 7 years and is portrayed as camp really is, a great place to spend the summer. Everyone who has ever gone to camp, wanted to go to camp, or has sent a child to camp should see this movie because it'll bring back wonderful memories for you and for your kids.\"\n",
      "1 b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n",
      "1 b\"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"\n",
      "1 b\"I first saw this movie here in the U.K. in December 1989 when Central TV broadcast it. I still have the video tape, although worn out (over the years many friends and family members have borrowed it and have also been chilled by it!). <br /><br />Anyway, I remember coming home that night, grabbing a Christmas tipple, switching the lights out and watching what was advertised as a 'Christmas Ghost Story'. Even now I remember certain scenes that still send the hairs on my neck standing on-end... <br /><br />I have seen some comments on the movie which say it's not this and not that...I think those people get scared by Friday 13th and the like, stalk and slash rammel, which are laughable. This is a 'traditional' ghost story; there is no big budget action or special effects...no swearing, no blood, no gratuitous sex scenes, no chainsaws or guns etc...So how refreshing!!!! It's atmospheric. IF you like chilling horror, well written, well acted and with a genuinely scary atmosphere, this is the movie for you. I like the original horrors; only last night I saw the original Haunting and that is a superb movie. Very atmospheric again - and so is The Woman In Black. The end of the movie differs to the book, but still very good. I recommend it. Try it...you *will* like it if you like traditional ghost stories...SO...turn off the lights, turn up the fire, lock the doors, grab a drink...and enjoy... :)\"\n",
      "1 b'I had lost faith in Sooraj R. Barjatya after the movie Main Prem Ki Deewani hoon, then a year back now I saw promos for Vivah which looked good. But I didn\\'t want to waste my hard earned money watching it in cinema. When the film first came out on DVD I rented it and watched and I loved the movie and took back my words for Sooraj. I just finished watching it yesterday again and this time I thought I have to review this movie. Sooraj R. BarjatyaGot it right this time, okay I was not a huge fan of Hum App Ke Hai Kaun. But I have always loved Manie Pyar kiya, after Manie Pyar kiya to me I think Vivah is Barjatyas best work. I hardly ever cry in a movie but this movie made me feel like crying. If you have ever been in love before then there will be many moments that will touch you in this movie, the movie is just too sweet and will have you falling in love with it, my view a much underrated movie.<br /><br />The story of this movie you might call desi and very old times, but to me it seemed modern because the two couples which are getting an arrange marriage are aware it\\'s an old tradition. It\\'s done in present times, lots of people don\\'t believe in this arrange marriage, but I do. The journey between the engagement and wedding which will always be special and this movie shows it clearly. When Prem meets Poonam for the first time, they show it how it is and that\\'s reality and my parents where saying that\\'s how they got married and it showed it in a way which is so real yes people the way Prem and Poonam meet in this movie is how most marriages happen. It was a very sweet, you feel nervous yet excited, the song \"Do Ajnabe\" shows that very well. Getting back to the story yes it\\'s a journey which you soon get glued to between Prem and Poonam (Shahid Kapoor and Amrita Rao) and there families. A twist occurs in this movie which is really good, the last 30mins you all will be reaching for the tissue box.<br /><br />What makes this film so amazing is the chemistry between Prem and Poonam, how they fall for each other is too sweet. Simple boy and Simple Girl, when they first meet during and after the song \"Do Anjane Ajnabe\" It\\'s very sweet to watch, She hardly says anything and Prem does all the talking being honest with her about his past and the girl he liked and him smoking. Then it leads on to them all having a family trip and then that\\'s when they really do fall for each other. It makes you just want to watch the couple and watch all the sweet moments they have. Another factor is that Poonam chichi is really mean to her and you feel sorry for Poonam because she has been treated bad and makes you want to see her happy and when she finally finds happiness, you too start feeling happy with her the movie basically makes you fall in love with Poonam more then just Prem. When she finds happiness through Prem you want her to stay happy and also hope nothing goes wrong because the character is shown as a sweet simple girl. Which brings me to performances and Amrita Roa as Poonam is amazing in the movie, her best work till date you will fall in love with this innocent character and root her on to find happiness. Shahid Kapoor as Prem is amazing too, he is Poonam support in the film, he is her happiness the movie, together they share an amazing chemistry and I have never seen a cuter couple since SRK and Kajol. If Ishq Vishk didn\\'t touch you to telling you how cute they are together this surely will. \"Mujhe Haq hai\" the song and before that is amazing chemistry they show. Scenes which touched me was when Prem takes Poonam to his room and shows her that\\'s where they will be staying and he opens her up and they have a moment between them which is too sweet. Again if you have ever been in love with someone that much these scenes you can defiantly connect to. The film is just the sweetest thing you will see ever.<br /><br />The direction is spot on, to me a good movie is basically something that can pull me in and stop me believe for this hours what is being seen here is fake and there is a camera filing them. To me this film pulled me in and for those three hours I felt really connected to the movie. The songs you will only truly like when you have seen the movie as they are songs placed in the situation after I saw the movie I been playing the songs non stop! The music is amazing, the story is simply amazing too what more can I ask for?<br /><br />What I can finally say it, rarely do we get a movie that makes us feel good, this movie after you have seen it will make you feel really good and make you want to be a better person. Its basically the sweetest journey ever, its basically showing you they journey between engagement and marriage and many people say it\\'s the bestest part of your life\\xc2\\x85Well this movie actually shows you way do people actually say that? Why do people actually say that the journey is just that amazing! Watch this movie and you will find out why the journey is amazing!'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:54:53.040531Z",
     "start_time": "2021-10-16T11:54:53.026119Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:56:57.674663Z",
     "start_time": "2021-10-16T11:56:57.656359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:57:46.530201Z",
     "start_time": "2021-10-16T11:57:46.500574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.036384  , -0.04701727, -0.0426507 ,  0.00608152,  0.04362835],\n",
       "       [-0.04367859,  0.03382819,  0.03753097, -0.00923725,  0.04230264],\n",
       "       [ 0.01993679, -0.0214172 ,  0.03195946, -0.0386781 ,  0.04855793]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T11:59:03.964768Z",
     "start_time": "2021-10-16T11:59:03.952169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:03:06.322312Z",
     "start_time": "2021-10-16T12:03:04.395297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:06:15.621322Z",
     "start_time": "2021-10-16T12:06:15.548079Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:06:27.868633Z",
     "start_time": "2021-10-16T12:06:27.854844Z"
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:06:43.394234Z",
     "start_time": "2021-10-16T12:06:43.303044Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:07:43.326782Z",
     "start_time": "2021-10-16T12:07:13.738234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 3s 6ms/step - loss: 0.6230 - accuracy: 0.5671 - val_loss: 0.5014 - val_accuracy: 0.7008\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4012 - accuracy: 0.8150 - val_loss: 0.3894 - val_accuracy: 0.8058\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3089 - accuracy: 0.8645 - val_loss: 0.3718 - val_accuracy: 0.8178\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2608 - accuracy: 0.8905 - val_loss: 0.3775 - val_accuracy: 0.8198\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2267 - accuracy: 0.9068 - val_loss: 0.3946 - val_accuracy: 0.8206\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1999 - accuracy: 0.9201 - val_loss: 0.4188 - val_accuracy: 0.8198\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1773 - accuracy: 0.9310 - val_loss: 0.4492 - val_accuracy: 0.8150\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1577 - accuracy: 0.9403 - val_loss: 0.4843 - val_accuracy: 0.8080\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1404 - accuracy: 0.9486 - val_loss: 0.5233 - val_accuracy: 0.8052\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1247 - accuracy: 0.9556 - val_loss: 0.5659 - val_accuracy: 0.8032\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1103 - accuracy: 0.9614 - val_loss: 0.6117 - val_accuracy: 0.7982\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0971 - accuracy: 0.9682 - val_loss: 0.6601 - val_accuracy: 0.7932\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0849 - accuracy: 0.9748 - val_loss: 0.7114 - val_accuracy: 0.7920\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0737 - accuracy: 0.9806 - val_loss: 0.7641 - val_accuracy: 0.7892\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0635 - accuracy: 0.9842 - val_loss: 0.8177 - val_accuracy: 0.7878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb5529a5f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:08:48.221970Z",
     "start_time": "2021-10-16T12:08:48.216373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:09:24.432356Z",
     "start_time": "2021-10-16T12:09:21.388561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-47a988eafa89e890\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-47a988eafa89e890\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:12:49.929397Z",
     "start_time": "2021-10-16T12:12:49.875981Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:14:07.869158Z",
     "start_time": "2021-10-16T12:14:07.717977Z"
    }
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T12:14:55.296164Z",
     "start_time": "2021-10-16T12:14:55.288050Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ml-learn)",
   "language": "python",
   "name": "pycharm-86d2ed7d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
