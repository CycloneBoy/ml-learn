{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-17T13:56:07.116621Z","iopub.execute_input":"2021-08-17T13:56:07.116944Z","iopub.status.idle":"2021-08-17T13:56:07.147814Z","shell.execute_reply.started":"2021-08-17T13:56:07.116915Z","shell.execute_reply":"2021-08-17T13:56:07.147020Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/train_eval.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/run.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/LICENSE\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/README.md\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/utils.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/ERNIE_pretrain/README.md\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/test.txt\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/train.txt\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/dev.txt\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/class.txt\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/convert_transfo_xl_checkpoint_to_pytorch.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/modeling_gpt2.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/convert_openai_checkpoint_to_pytorch.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/modeling.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/file_utils.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/tokenization.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/convert_tf_checkpoint_to_pytorch.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/tokenization_transfo_xl.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/modeling_transfo_xl_utilities.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/modeling_openai.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/optimization.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/optimization_openai.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/tokenization_gpt2.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/tokenization_openai.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/convert_gpt2_checkpoint_to_pytorch.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/__init__.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/modeling_transfo_xl.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/pytorch_pretrained/__main__.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/models/bert_DPCNN.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/models/bert.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/models/ERNIE.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/models/bert_RNN.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/models/bert_RCNN.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/models/bert_CNN.py\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/bert_pretrain/bert_config.json\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/bert_pretrain/README.md\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/bert_pretrain/pytorch_model.bin\n/kaggle/input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/bert_pretrain/vocab.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('../input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T13:56:10.061237Z","iopub.execute_input":"2021-08-17T13:56:10.061557Z","iopub.status.idle":"2021-08-17T13:56:10.068337Z","shell.execute_reply.started":"2021-08-17T13:56:10.061529Z","shell.execute_reply":"2021-08-17T13:56:10.067239Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"../input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/test.txt\n../input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/train.txt\n../input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/dev.txt\n../input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews/data/class.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT文本分类","metadata":{}},{"cell_type":"markdown","source":"\n## 定义模型","metadata":{}},{"cell_type":"markdown","source":"### bert model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer, BertConfig\n\nclass UserBertConfig(object):\n    \"\"\"配置参数\"\"\"\n\n    def __init__(self, dataset,name = 'bert-base-chinese'):\n        self.model_name = 'bert_v1_'+name\n        self.train_path = dataset + '/data/train.txt'  # 训练集\n        self.dev_path = dataset + '/data/dev.txt'  # 验证集\n        self.test_path = dataset + '/data/test.txt'  # 测试集\n        self.class_list = [x.strip() for x in open(\n            dataset + '/data/class.txt').readlines()]  # 类别名单\n        self.log_path = './log/' + self.model_name\n        self.save_path = dataset + './model/' + self.model_name + '.ckpt'  # 模型训练结果\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 设备\n\n        self.require_improvement = 1000  # 若超过1000batch效果还没提升，则提前结束训练\n        self.num_classes = len(self.class_list)  # 类别数\n        self.num_epochs = 3  # epoch数\n        self.batch_size = 128  # mini-batch大小\n        self.pad_size = 32  # 每句话处理成的长度(短填长切)\n        self.learning_rate = 5e-5  # 学习率\n        self.bert_path = './bert_pretrain'\n        self.pretrain_name = name\n        self.tokenizer = BertTokenizer.from_pretrained(self.pretrain_name)\n        self.hidden_size = 768\n        self.model_config = BertConfig.from_pretrained(self.pretrain_name, output_hidden_states=True)\n\n\nclass UserBertModel(nn.Module):\n\n    def __init__(self, config):\n        super(UserBertModel, self).__init__()\n\n#         mirror='tuna',\n        self.bert = BertModel.from_pretrained(config.pretrain_name,  config=config.model_config)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n\n    def forward(self, x):\n        context = x[0]  # 输入的句子\n        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n        bertout = self.bert(context, attention_mask=mask)\n        out = self.fc(bertout.pooler_output )\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T13:56:18.593753Z","iopub.execute_input":"2021-08-17T13:56:18.594069Z","iopub.status.idle":"2021-08-17T13:56:18.606300Z","shell.execute_reply.started":"2021-08-17T13:56:18.594035Z","shell.execute_reply":"2021-08-17T13:56:18.605202Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### chinese-bert-wwm","metadata":{}},{"cell_type":"markdown","source":"### utils.py","metadata":{}},{"cell_type":"code","source":"# \n# coding: UTF-8\nimport torch\nfrom tqdm import tqdm\nimport time\nfrom datetime import timedelta\n\nPAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n\n\ndef build_dataset(config):\n\n    def load_dataset(path, pad_size=32):\n        contents = []\n        with open(path, 'r', encoding='UTF-8') as f:\n            for line in tqdm(f):\n                lin = line.strip()\n                if not lin:\n                    continue\n                content, label = lin.split('\\t')\n                token = config.tokenizer.tokenize(content)\n                token = [CLS] + token\n                seq_len = len(token)\n                mask = []\n                token_ids = config.tokenizer.convert_tokens_to_ids(token)\n\n                if pad_size:\n                    if len(token) < pad_size:\n                        mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n                        token_ids += ([0] * (pad_size - len(token)))\n                    else:\n                        mask = [1] * pad_size\n                        token_ids = token_ids[:pad_size]\n                        seq_len = pad_size\n                contents.append((token_ids, int(label), seq_len, mask))\n        return contents\n    train = load_dataset(config.train_path, config.pad_size)\n    dev = load_dataset(config.dev_path, config.pad_size)\n    test = load_dataset(config.test_path, config.pad_size)\n    return train, dev, test\n\n\nclass DatasetIterater(object):\n    def __init__(self, batches, batch_size, device):\n        self.batch_size = batch_size\n        self.batches = batches\n        self.n_batches = len(batches) // batch_size\n        self.residue = False  # 记录batch数量是否为整数\n        if len(batches) % self.n_batches != 0:\n            self.residue = True\n        self.index = 0\n        self.device = device\n\n    def _to_tensor(self, datas):\n        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n\n        # pad前的长度(超过pad_size的设为pad_size)\n        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n        return (x, seq_len, mask), y\n\n    def __next__(self):\n        if self.residue and self.index == self.n_batches:\n            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n        elif self.index >= self.n_batches:\n            self.index = 0\n            raise StopIteration\n        else:\n            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        if self.residue:\n            return self.n_batches + 1\n        else:\n            return self.n_batches\n\n\ndef build_iterator(dataset, config):\n    iter = DatasetIterater(dataset, config.batch_size, config.device)\n    return iter\n\n\ndef get_time_dif(start_time):\n    \"\"\"获取已使用时间\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T13:56:39.324774Z","iopub.execute_input":"2021-08-17T13:56:39.325269Z","iopub.status.idle":"2021-08-17T13:56:39.354028Z","shell.execute_reply.started":"2021-08-17T13:56:39.325224Z","shell.execute_reply":"2021-08-17T13:56:39.352752Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### train_eval.py","metadata":{}},{"cell_type":"code","source":"# coding: UTF-8\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tensorboardX import SummaryWriter\nfrom sklearn import metrics\nimport time\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n\n\n# 权重初始化，默认xavier\ndef init_network(model, method='xavier', exclude='embedding', seed=123):\n    for name, w in model.named_parameters():\n        if exclude not in name:\n            if len(w.size()) < 2:\n                continue\n            if 'weight' in name:\n                if method == 'xavier':\n                    nn.init.xavier_normal_(w)\n                elif method == 'kaiming':\n                    nn.init.kaiming_normal_(w)\n                else:\n                    nn.init.normal_(w)\n            elif 'bias' in name:\n                nn.init.constant_(w, 0)\n            else:\n                pass\n\n\ndef train(config, model, train_iter, dev_iter, test_iter):\n    start_time = time.time()\n    model.train()\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    # optimizer_grouped_parameters = [\n    #     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    #     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n\n    max_grad_norm = 1.0\n    warmup = 0.05\n    num_training_steps = len(train_iter) * config.num_epochs\n    num_warmup_steps = num_training_steps * 0.05\n    warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n\n    # optimizer = BertAdam(optimizer_grouped_parameters,\n    #                      lr=config.learning_rate,\n    #                      warmup=0.05,\n    #                      t_total=len(train_iter) * config.num_epochs)\n\n    optimizer = AdamW(model.parameters(),lr=config.learning_rate,correct_bias=False)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=num_training_steps)  # PyTorch scheduler\n    total_batch = 0  # 记录进行到多少batch\n    dev_best_loss = float('inf')\n    last_improve = 0  # 记录上次验证集loss下降的batch数\n    flag = False  # 记录是否很久没有效果提升\n    writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n    model.train()\n    for epoch in range(config.num_epochs):\n        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n        for i, (trains, labels) in enumerate(train_iter):\n            outputs = model(trains)\n\n            model.zero_grad()\n            loss = F.cross_entropy(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # 梯度裁剪不再在AdamW中了(因此你可以毫无问题地使用放大器)\n\n            optimizer.step()\n            scheduler.step()\n            if total_batch % 100 == 0:\n                # 每多少轮输出在训练集和验证集上的效果\n                true = labels.data.cpu()\n                predic = torch.max(outputs.data, 1)[1].cpu()\n                train_acc = metrics.accuracy_score(true, predic)\n                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n                if dev_loss < dev_best_loss:\n                    dev_best_loss = dev_loss\n                    print(f\"save onece model:  {i}\")\n                    torch.save(model.state_dict(), \"bert_v1.ckpt\")\n                    improve = '*'\n                    last_improve = total_batch\n                else:\n                    improve = ''\n                time_dif = get_time_dif(start_time)\n                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n                model.train()\n            total_batch += 1\n            if total_batch - last_improve > config.require_improvement:\n                # 验证集loss超过1000batch没下降，结束训练\n                print(\"No optimization for a long time, auto-stopping...\")\n                flag = True\n                break\n        if flag:\n            break\n    test(config, model, test_iter)\n\n\ndef test(config, model, test_iter):\n    # test\n    model.load_state_dict(torch.load(config.save_path))\n    model.eval()\n    start_time = time.time()\n    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n    print(msg.format(test_loss, test_acc))\n    print(\"Precision, Recall and F1-Score...\")\n    print(test_report)\n    print(\"Confusion Matrix...\")\n    print(test_confusion)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n\ndef evaluate(config, model, data_iter, test=False):\n    model.eval()\n    loss_total = 0\n    predict_all = np.array([], dtype=int)\n    labels_all = np.array([], dtype=int)\n    with torch.no_grad():\n        for texts, labels in data_iter:\n            outputs = model(texts)\n            loss = F.cross_entropy(outputs, labels)\n            loss_total += loss\n            labels = labels.data.cpu().numpy()\n            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n            labels_all = np.append(labels_all, labels)\n            predict_all = np.append(predict_all, predic)\n\n    acc = metrics.accuracy_score(labels_all, predict_all)\n    if test:\n        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n        confusion = metrics.confusion_matrix(labels_all, predict_all)\n        return acc, loss_total / len(data_iter), report, confusion\n    return acc, loss_total / len(data_iter)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T13:56:44.907656Z","iopub.execute_input":"2021-08-17T13:56:44.907983Z","iopub.status.idle":"2021-08-17T13:56:44.934786Z","shell.execute_reply.started":"2021-08-17T13:56:44.907951Z","shell.execute_reply":"2021-08-17T13:56:44.933771Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### main.py","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nfrom importlib import import_module\nimport argparse\n\n\ndef main(run_model= UserBertModel,run_config=UserBertConfig,pretrained_name='bert-base-chinese',\n         dataset=\"../input/bert3/Bert-Chinese-Text-Classification-Pytorch-master/THUCNews\"):\n#     dataset = 'THUCNews'  # 数据集\n\n    config = run_config(dataset,pretrained_name)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed_all(1)\n    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n\n    start_time = time.time()\n    print(f\"device : {config.device}\")\n    print(f\"model : {pretrained_name}\")\n    print(f\"config : {config}\")\n    print(\"Loading data...\")\n    train_data, dev_data, test_data = build_dataset(config)\n    train_iter = build_iterator(train_data, config)\n    dev_iter = build_iterator(dev_data, config)\n    test_iter = build_iterator(test_data, config)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n    # train\n    model = run_model(config).to(config.device)\n    print(model)\n    train(config, model, train_iter, dev_iter, test_iter)\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T13:56:47.563866Z","iopub.execute_input":"2021-08-17T13:56:47.564252Z","iopub.status.idle":"2021-08-17T13:56:47.571864Z","shell.execute_reply.started":"2021-08-17T13:56:47.564218Z","shell.execute_reply":"2021-08-17T13:56:47.570869Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## 执行Main ","metadata":{}},{"cell_type":"code","source":"# main(pretrained_name='bert-base-chinese')\nmain(pretrained_name='hfl/chinese-bert-wwm')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T13:56:50.918806Z","iopub.execute_input":"2021-08-17T13:56:50.919158Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"399it [00:00, 3982.10it/s]","output_type":"stream"},{"name":"stdout","text":"device : cuda\nmodel : hfl/chinese-bert-wwm\nconfig : <__main__.UserBertConfig object at 0x7f3f507eefd0>\nLoading data...\n","output_type":"stream"},{"name":"stderr","text":"180000it [00:48, 3730.01it/s]\n10000it [00:02, 3935.64it/s]\n10000it [00:02, 3893.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Time usage: 0:00:53\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a48f86bf5ad40e080fbe31222c99f4b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"UserBertModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (fc): Linear(in_features=768, out_features=10, bias=True)\n)\nEpoch [1/3]\nsave onece model:  0\nIter:      0,  Train Loss:   2.5,  Train Acc:  5.47%,  Val Loss:   2.4,  Val Acc:  7.95%,  Time: 0:00:11 *\nsave onece model:  100\nIter:    100,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.11%,  Time: 0:01:00 *\nsave onece model:  200\nIter:    200,  Train Loss:  0.49,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 89.72%,  Time: 0:01:49 *\nsave onece model:  300\nIter:    300,  Train Loss:   0.3,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 90.23%,  Time: 0:02:38 *\nsave onece model:  400\nIter:    400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:   0.3,  Val Acc: 90.99%,  Time: 0:03:26 *\nsave onece model:  500\nIter:    500,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:   0.3,  Val Acc: 91.23%,  Time: 0:04:15 *\nsave onece model:  600\nIter:    600,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.28,  Val Acc: 91.45%,  Time: 0:05:04 *\nsave onece model:  700\nIter:    700,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.94%,  Time: 0:05:53 *\nsave onece model:  800\nIter:    800,  Train Loss:  0.19,  Train Acc: 94.53%,  Val Loss:  0.25,  Val Acc: 92.30%,  Time: 0:06:41 *\nsave onece model:  900\nIter:    900,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.25,  Val Acc: 92.50%,  Time: 0:07:30 *\nsave onece model:  1000\nIter:   1000,  Train Loss:  0.15,  Train Acc: 92.97%,  Val Loss:  0.25,  Val Acc: 92.33%,  Time: 0:08:19 *\nsave onece model:  1100\nIter:   1100,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.98%,  Time: 0:09:07 *\nIter:   1200,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.22,  Val Acc: 92.88%,  Time: 0:09:55 \nsave onece model:  1300\nIter:   1300,  Train Loss:  0.24,  Train Acc: 89.06%,  Val Loss:  0.21,  Val Acc: 93.30%,  Time: 0:10:44 *\nsave onece model:  1400\nIter:   1400,  Train Loss:  0.33,  Train Acc: 90.62%,  Val Loss:  0.21,  Val Acc: 93.60%,  Time: 0:11:32 *\nEpoch [2/3]\nIter:   1500,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.21,  Val Acc: 93.42%,  Time: 0:12:20 \nIter:   1600,  Train Loss:  0.27,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.12%,  Time: 0:13:07 \nsave onece model:  293\nIter:   1700,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.21,  Val Acc: 93.58%,  Time: 0:13:56 *\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}