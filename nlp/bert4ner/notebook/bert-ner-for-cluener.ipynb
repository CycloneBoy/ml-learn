{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### 本篇博客希望展示如何基于transformers提供的功能进行模型的开发，减少代码量，提高开发速度。","metadata":{}},{"cell_type":"markdown","source":"# 定义全局变量","metadata":{}},{"cell_type":"code","source":"CLUENER_DATASET_DIR = \"../input/business-privacy-identify1/Business_Privacy_Identify/data/clue\"\n\n# 数据集路径\nDATASET_DIR = CLUENER_DATASET_DIR\n\n# BERT 模型   \"bert-base-chinese\"\nBERT_MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:08.096404Z","iopub.execute_input":"2021-08-20T04:22:08.096773Z","iopub.status.idle":"2021-08-20T04:22:08.108239Z","shell.execute_reply.started":"2021-08-20T04:22:08.096686Z","shell.execute_reply":"2021-08-20T04:22:08.107156Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport warnings\nimport torch.nn as nn\nimport numpy as np\nimport json\n\nfrom torch import Tensor\nfrom typing import List, Dict\nfrom dataclasses import dataclass, field\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers.file_utils import logger, logging\nfrom transformers.trainer_utils import EvalPrediction\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom transformers import TrainingArguments, Trainer, BertTokenizer, BertModel, BertPreTrainedModel\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:08.109660Z","iopub.execute_input":"2021-08-20T04:22:08.110030Z","iopub.status.idle":"2021-08-20T04:22:14.784559Z","shell.execute_reply.started":"2021-08-20T04:22:08.109995Z","shell.execute_reply":"2021-08-20T04:22:14.783726Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 一、定义参数","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass ModelArguments:\n    use_lstm: bool = field(default=True, metadata={\"help\": \"是否使用LSTM\"})\n    lstm_hidden_size: int = field(default=500, metadata={\"help\": \"LSTM隐藏层输出的维度\"})\n    lstm_layers: int = field(default=1, metadata={\"help\": \"堆叠LSTM的层数\"})\n    lstm_dropout: float = field(default=0.5, metadata={\"help\": \"LSTM的dropout\"})\n    hidden_dropout: float = field(default=0.5, metadata={\"help\": \"预训练模型输出向量表示的dropout\"})\n    ner_num_labels: int = field(default=34, metadata={\"help\": \"需要预测的标签数量\"})\n\n\n@dataclass\nclass OurTrainingArguments:\n    checkpoint_dir: str = field(default=\"./models/checkpoints\", metadata={\"help\": \"训练过程中的checkpoints的保存路径\"})\n    best_dir: str = field(default=\"./models/best\", metadata={\"help\": \"最优模型的保存路径\"})\n    do_eval: bool = field(default=True, metadata={\"help\": \"是否在训练时进行评估\"})\n    do_predict: bool = field(default=True, metadata={\"help\": \"是否在训练时进行预测\"})\n    epoch: int = field(default=5, metadata={\"help\": \"训练的epoch\"})\n    train_batch_size: int = field(default=8, metadata={\"help\": \"训练时的batch size\"})\n    eval_batch_size: int = field(default=8, metadata={\"help\": \"评估时的batch size\"})\n    bert_model_name: str = field(default=BERT_MODEL_NAME, metadata={\"help\": \"BERT模型名称\"})\n\n\n@dataclass\nclass DataArguments:\n    train_file: str = field(default=DATASET_DIR +\"/train.json\", metadata={\"help\": \"训练数据的路径\"})\n    dev_file: str = field(default=DATASET_DIR +\"/dev.json\", metadata={\"help\": \"测试数据的路径\"})\n    test_file: str = field(default=DATASET_DIR +\"/test.json\", metadata={\"help\": \"测试数据的路径\"})","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:14.786331Z","iopub.execute_input":"2021-08-20T04:22:14.786641Z","iopub.status.idle":"2021-08-20T04:22:14.800075Z","shell.execute_reply.started":"2021-08-20T04:22:14.786613Z","shell.execute_reply":"2021-08-20T04:22:14.799191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 二、读取数据","metadata":{}},{"cell_type":"markdown","source":"这里定义了一个用于保存数据的数据结构，这样的方法能够提高代码的可阅读性。","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Example:\n    text: List[str] # ner的文本\n    label: List[str] = None # ner的标签\n\n    def __post_init__(self):\n        if self.label:\n            assert len(self.text) == len(self.label)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:14.801638Z","iopub.execute_input":"2021-08-20T04:22:14.802128Z","iopub.status.idle":"2021-08-20T04:22:14.815961Z","shell.execute_reply.started":"2021-08-20T04:22:14.802091Z","shell.execute_reply":"2021-08-20T04:22:14.815020Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"定义将文件中的ner数据保存为Example列表的函数","metadata":{}},{"cell_type":"code","source":"# 读取数据集:json 格式\ndef read_json(input_file):\n    \"\"\"read dataset \"\"\"\n    lines = []\n    with open(input_file, 'r') as f:\n        for line in f:\n            line = json.loads(line.strip())\n            text = line['text']\n            label_entities = line.get('label', None)\n            words = list(text)\n            labels = ['O'] * len(words)\n            if label_entities is not None:\n                for key, value in label_entities.items():\n                    for sub_name, sub_index in value.items():\n                        for start_index, end_index in sub_index:\n                            assert ''.join(words[start_index:end_index + 1]) == sub_name\n                            if start_index == end_index:\n                                labels[start_index] = 'S-' + key\n                            else:\n                                labels[start_index] = 'B-' + key\n                                labels[start_index + 1:end_index + 1] = ['I-' + key] * (len(sub_name) - 1)\n            lines.append({\"words\": words, \"labels\": labels})\n    return lines\n\ndef read_dataset_json(input_file):\n    \"\"\" 读取数据集:json 格式  \"\"\"\n    examples = []\n    lines = read_json(input_file)\n    for line in lines:\n        examples.append(Example(line[\"words\"], line[\"labels\"]))\n\n    return examples\n\n# 读取数据集:json 格式\ndef read_dataset_txt(input_file):\n    \"\"\"read dataset \"\"\"\n    examples = []\n    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n        text = []\n        label = []\n        for line in file:\n            line = line.strip()\n            # 一条文本结束\n            if len(line) == 0:\n                examples.append(Example(text, label))\n                text = []\n                label = []\n                continue\n            text.append(line.split()[0])\n            label.append(line.split()[1])\n    return examples","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:14.819843Z","iopub.execute_input":"2021-08-20T04:22:14.820192Z","iopub.status.idle":"2021-08-20T04:22:14.835324Z","shell.execute_reply.started":"2021-08-20T04:22:14.820164Z","shell.execute_reply":"2021-08-20T04:22:14.834273Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n\ndef read_data(path, data_type=\"json\"):\n    examples = None\n    if data_type == 'txt':\n        examples = read_dataset_txt(path)\n    elif data_type == \"json\":\n        examples = read_dataset_json(path)\n\n    return examples\n\ntrain_data = read_data(DATASET_DIR +\"/train.json\")\neval_data = read_data(DATASET_DIR +\"/dev.json\")\nprint(train_data[0])\n\n\nfor i in range(10):\n  print(train_data[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:14.837139Z","iopub.execute_input":"2021-08-20T04:22:14.837405Z","iopub.status.idle":"2021-08-20T04:22:15.126061Z","shell.execute_reply.started":"2021-08-20T04:22:14.837369Z","shell.execute_reply":"2021-08-20T04:22:15.125177Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Example(text=['浙', '商', '银', '行', '企', '业', '信', '贷', '部', '叶', '老', '桂', '博', '士', '则', '从', '另', '一', '个', '角', '度', '对', '五', '道', '门', '槛', '进', '行', '了', '解', '读', '。', '叶', '老', '桂', '认', '为', '，', '对', '目', '前', '国', '内', '商', '业', '银', '行', '而', '言', '，'], label=['B-company', 'I-company', 'I-company', 'I-company', 'O', 'O', 'O', 'O', 'O', 'B-name', 'I-name', 'I-name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['浙', '商', '银', '行', '企', '业', '信', '贷', '部', '叶', '老', '桂', '博', '士', '则', '从', '另', '一', '个', '角', '度', '对', '五', '道', '门', '槛', '进', '行', '了', '解', '读', '。', '叶', '老', '桂', '认', '为', '，', '对', '目', '前', '国', '内', '商', '业', '银', '行', '而', '言', '，'], label=['B-company', 'I-company', 'I-company', 'I-company', 'O', 'O', 'O', 'O', 'O', 'B-name', 'I-name', 'I-name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['生', '生', '不', '息', 'C', 'S', 'O', 'L', '生', '化', '狂', '潮', '让', '你', '填', '弹', '狂', '扫'], label=['O', 'O', 'O', 'O', 'B-game', 'I-game', 'I-game', 'I-game', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['那', '不', '勒', '斯', 'v', 's', '锡', '耶', '纳', '以', '及', '桑', '普', 'v', 's', '热', '那', '亚', '之', '上', '呢', '？'], label=['B-organization', 'I-organization', 'I-organization', 'I-organization', 'O', 'O', 'B-organization', 'I-organization', 'I-organization', 'O', 'O', 'B-organization', 'I-organization', 'O', 'O', 'B-organization', 'I-organization', 'I-organization', 'O', 'O', 'O', 'O'])\nExample(text=['加', '勒', '比', '海', '盗', '3', '：', '世', '界', '尽', '头', '》', '的', '去', '年', '同', '期', '成', '绩', '死', '死', '甩', '在', '身', '后', '，', '后', '者', '则', '即', '将', '赶', '超', '《', '变', '形', '金', '刚', '》', '，'], label=['B-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'O'])\nExample(text=['布', '鲁', '京', '斯', '研', '究', '所', '桑', '顿', '中', '国', '中', '心', '研', '究', '部', '主', '任', '李', '成', '说', '，', '东', '亚', '的', '和', '平', '与', '安', '全', '，', '是', '美', '国', '的', '“', '核', '心', '利', '益', '”', '之', '一', '。'], label=['B-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'B-position', 'I-position', 'I-position', 'I-position', 'I-position', 'B-name', 'I-name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-address', 'I-address', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['目', '前', '主', '赞', '助', '商', '暂', '时', '空', '缺', '，', '他', '们', '的', '球', '衣', '上', '印', '的', '是', '“', 'u', 'n', 'i', 'c', 'e', 'f', '”', '（', '联', '合', '国', '儿', '童', '基', '金', '会', '）', '，', '是', '公', '益', '性', '质', '的', '广', '告', '；'], label=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'O', 'O', 'B-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'I-organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['此', '数', '据', '换', '算', '成', '亚', '洲', '盘', '罗', '马', '客', '场', '可', '让', '平', '半', '低', '水', '。'], label=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-organization', 'I-organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['你', '们', '是', '最', '棒', '的', '!', '#', '英', '雄', '联', '盟', 'd', '学', 's', 'a', 'n', 'c', 'h', 'e', 'z', '创', '作', '的', '原', '声', '王'], label=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-game', 'I-game', 'I-game', 'I-game', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['除', '了', '吴', '湖', '帆', '时', '现', '精', '彩', '，', '吴', '待', '秋', '、', '吴', '子', '深', '、', '冯', '超', '然', '已', '然', '归', '入', '二', '三', '流', '了', '，'], label=['O', 'O', 'B-name', 'I-name', 'I-name', 'O', 'O', 'O', 'O', 'O', 'B-name', 'I-name', 'I-name', 'O', 'B-name', 'I-name', 'I-name', 'O', 'B-name', 'I-name', 'I-name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\nExample(text=['在', '豪', '门', '被', '多', '线', '作', '战', '拖', '累', '时', '，', '正', '是', '他', '们', '悄', '悄', '追', '赶', '上', '来', '的', '大', '好', '时', '机', '。', '重', '新', '找', '回', '全', '队', '的', '凝', '聚', '力', '是', '拉', '科', '赢', '球', '的', '资', '本', '。'], label=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-organization', 'I-organization', 'O', 'O', 'O', 'O', 'O', 'O'])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"加载标签数据并分配对于的id","metadata":{}},{"cell_type":"code","source":"def get_labels_from_list():\n    \"CLUENER TAGS\"\n    return [\"<pad>\", \"B-address\", \"B-book\", \"B-company\", 'B-game', 'B-government', 'B-movie', 'B-name',\n            'B-organization', 'B-position', 'B-scene', \"I-address\",\n            \"I-book\", \"I-company\", 'I-game', 'I-government', 'I-movie', 'I-name',\n            'I-organization', 'I-position', 'I-scene',\n            \"S-address\", \"S-book\", \"S-company\", 'S-game', 'S-government', 'S-movie',\n            'S-name', 'S-organization', 'S-position',\n            'S-scene', 'O', \"<start>\", \"<eos>\"]\n\n\ndef load_tag_from_file(path):\n    with open(path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n        tag2id = {tag.strip(): idx for idx, tag in enumerate(lines)}\n        id2tag = {idx: tag for tag, idx in tag2id.items()}\n    return tag2id, id2tag\n\n\ndef load_tag(path=None):\n    if path is not None:\n        tag2id, id2tag = load_tag_from_file(path)\n    else:\n        tags = get_labels_from_list()\n\n        id2tag = {i: label for i, label in enumerate(tags)}\n        tag2id = {label: i for i, label in enumerate(tags)}\n\n    return tag2id, id2tag\n\n\ntag2id, id2tag = load_tag()\nprint(tag2id)\nprint(id2tag)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:15.127335Z","iopub.execute_input":"2021-08-20T04:22:15.127681Z","iopub.status.idle":"2021-08-20T04:22:15.139551Z","shell.execute_reply.started":"2021-08-20T04:22:15.127644Z","shell.execute_reply":"2021-08-20T04:22:15.138585Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'<pad>': 0, 'B-address': 1, 'B-book': 2, 'B-company': 3, 'B-game': 4, 'B-government': 5, 'B-movie': 6, 'B-name': 7, 'B-organization': 8, 'B-position': 9, 'B-scene': 10, 'I-address': 11, 'I-book': 12, 'I-company': 13, 'I-game': 14, 'I-government': 15, 'I-movie': 16, 'I-name': 17, 'I-organization': 18, 'I-position': 19, 'I-scene': 20, 'S-address': 21, 'S-book': 22, 'S-company': 23, 'S-game': 24, 'S-government': 25, 'S-movie': 26, 'S-name': 27, 'S-organization': 28, 'S-position': 29, 'S-scene': 30, 'O': 31, '<start>': 32, '<eos>': 33}\n{0: '<pad>', 1: 'B-address', 2: 'B-book', 3: 'B-company', 4: 'B-game', 5: 'B-government', 6: 'B-movie', 7: 'B-name', 8: 'B-organization', 9: 'B-position', 10: 'B-scene', 11: 'I-address', 12: 'I-book', 13: 'I-company', 14: 'I-game', 15: 'I-government', 16: 'I-movie', 17: 'I-name', 18: 'I-organization', 19: 'I-position', 20: 'I-scene', 21: 'S-address', 22: 'S-book', 23: 'S-company', 24: 'S-game', 25: 'S-government', 26: 'S-movie', 27: 'S-name', 28: 'S-organization', 29: 'S-position', 30: 'S-scene', 31: 'O', 32: '<start>', 33: '<eos>'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"读取tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:15.142748Z","iopub.execute_input":"2021-08-20T04:22:15.143181Z","iopub.status.idle":"2021-08-20T04:22:19.403671Z","shell.execute_reply.started":"2021-08-20T04:22:15.143154Z","shell.execute_reply":"2021-08-20T04:22:19.402850Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e3b0e4afef4523b2b0d248edfacd00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5be89f436da48ac9bcc1f6c6895f5a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d9c74eb4b2489f9918646a4070f73e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa575873a24547b7a0cb9713d50c04f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec5fc827f214e9cba879854cb25fef5"}},"metadata":{}}]},{"cell_type":"markdown","source":"### 三、构建Dataset和collate_fn","metadata":{}},{"cell_type":"markdown","source":"构建Dataset","metadata":{}},{"cell_type":"code","source":"class NERDataset(Dataset):\n    def __init__(self, examples: List[Example], max_length=128,tokenizer=BertTokenizer.from_pretrained(BERT_MODEL_NAME)):\n        self.max_length = 512 if max_length > 512 else max_length\n        \"\"\"\n        1. 将文本的长度控制在max_length - 2，减2的原因是为[CLS]和[SEP]空出位置； \n        2. 将文本转换为id序列；\n        3. 将id序列转换为Tensor；\n        \"\"\"\n        self.texts = [torch.LongTensor(tokenizer.encode(example.text[: self.max_length - 2])) for example in examples]\n        self.labels = []\n        for example in examples:\n            label = example.label\n            \"\"\"\n            1. 将字符的label转换为对于的id；\n            2. 控制label的最长长度；\n            3. 添加开始位置和结束位置对应的标签，这里<start>对应输入中的[CLS],<eos>对于[SEP]；\n            4. 转换为Tensor；\n            \"\"\"\n            label = [tag2id[\"<start>\"]] + [tag2id[l] for l in label][: self.max_length - 2] + [tag2id[\"<eos>\"]]\n            self.labels.append(torch.LongTensor(label))\n        assert len(self.texts) == len(self.labels)\n        for text, label in zip(self.texts, self.labels):\n            assert len(text) == len(label)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        return {\n            \"input_ids\": self.texts[item],\n            \"labels\": self.labels[item]\n        }\n\ntrain_dataset = NERDataset(train_data)\neval_dataset = NERDataset(eval_data)\nprint(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:19.405318Z","iopub.execute_input":"2021-08-20T04:22:19.405691Z","iopub.status.idle":"2021-08-20T04:22:23.122661Z","shell.execute_reply.started":"2021-08-20T04:22:19.405650Z","shell.execute_reply":"2021-08-20T04:22:23.121533Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([ 101, 3851, 1555, 7213, 6121,  821,  689,  928, 6587, 6956, 1383, 5439,\n        3424, 1300, 1894, 1156,  794, 1369,  671,  702, 6235, 2428, 2190,  758,\n        6887, 7305, 3546, 6822, 6121,  749, 6237, 6438,  511, 1383, 5439, 3424,\n        6371,  711, 8024, 2190, 4680, 1184, 1744, 1079, 1555,  689, 7213, 6121,\n        5445, 6241, 8024,  102]), 'labels': tensor([32,  3, 13, 13, 13, 31, 31, 31, 31, 31,  7, 17, 17, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 33])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"定义collate_fn，collate_fn的作用在Dataloader生成batch数据时会被调用。\n这里的作用是对每个batch进行padding","metadata":{}},{"cell_type":"code","source":"def collate_fn(features) -> Dict[str, Tensor]:\n    batch_input_ids = [feature[\"input_ids\"] for feature in features]\n    batch_labels = [feature[\"labels\"] for feature in features]\n    batch_attentiton_mask = [torch.ones_like(feature[\"input_ids\"]) for feature in features]\n    # padding\n    batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    batch_labels = pad_sequence(batch_labels, batch_first=True, padding_value=tag2id[\"<pad>\"])\n    batch_attentiton_mask = pad_sequence(batch_attentiton_mask, batch_first=True, padding_value=0)\n    assert batch_input_ids.shape == batch_labels.shape\n    return {\"input_ids\": batch_input_ids, \"labels\": batch_labels, \"attention_mask\": batch_attentiton_mask}","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:23.124591Z","iopub.execute_input":"2021-08-20T04:22:23.125200Z","iopub.status.idle":"2021-08-20T04:22:23.134631Z","shell.execute_reply.started":"2021-08-20T04:22:23.125123Z","shell.execute_reply":"2021-08-20T04:22:23.133866Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"测试一下collate_fn","metadata":{}},{"cell_type":"code","source":"dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collate_fn)\nbatch = next(iter(dataloader))\nprint(batch.keys())\nprint(type(batch[\"input_ids\"]))\nprint(batch[\"input_ids\"].shape)\nprint(type(batch[\"labels\"]))\nprint(batch[\"labels\"].shape)\nprint(type(batch[\"attention_mask\"]))\nprint(batch[\"attention_mask\"].shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:23.136007Z","iopub.execute_input":"2021-08-20T04:22:23.136406Z","iopub.status.idle":"2021-08-20T04:22:23.168922Z","shell.execute_reply.started":"2021-08-20T04:22:23.136369Z","shell.execute_reply":"2021-08-20T04:22:23.168012Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'labels', 'attention_mask'])\n<class 'torch.Tensor'>\ntorch.Size([2, 45])\n<class 'torch.Tensor'>\ntorch.Size([2, 45])\n<class 'torch.Tensor'>\ntorch.Size([2, 45])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 四、定义一个评估函数","metadata":{}},{"cell_type":"code","source":"\ndef ner_metrics(eval_output: EvalPrediction) -> Dict[str, float]:\n    \"\"\"\n    该函数是回调函数，Trainer会在进行评估时调用该函数。\n    (如果使用Pycharm等IDE进行调试，可以使用断点的方法来调试该函数，该函数在进行评估时被调用)\n    \"\"\"\n    preds = eval_output.predictions\n    preds = np.argmax(preds, axis=-1).flatten()\n    labels = eval_output.label_ids.flatten()\n    # labels为0表示为<pad>，因此计算时需要去掉该部分\n    mask = labels != 0\n    preds = preds[mask]\n    labels = labels[mask]\n    metrics = dict()\n    metrics[\"f1\"] = f1_score(labels, preds, average=\"macro\")\n    metrics[\"precision\"] = precision_score(labels, preds, average=\"macro\")\n    metrics[\"recall\"] = recall_score(labels, preds, average=\"macro\")\n    # 必须以字典的形式返回，后面会用到字典的key\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:23.170177Z","iopub.execute_input":"2021-08-20T04:22:23.170702Z","iopub.status.idle":"2021-08-20T04:22:23.178007Z","shell.execute_reply.started":"2021-08-20T04:22:23.170663Z","shell.execute_reply":"2021-08-20T04:22:23.177143Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 五、构建模型\n+ 自定义的模型需要继承BertPreTrainedModel","metadata":{}},{"cell_type":"code","source":"class BertForNER(BertPreTrainedModel):\n    def __init__(self, config, *model_args, **model_kargs):\n        super().__init__(config) # 初始化父类(必要的步骤)\n        if \"model_args\" in model_kargs:\n            model_args = model_kargs[\"model_args\"]\n            \"\"\"\n            必须将额外的参数更新至self.config中，这样在调用save_model保存模型时才会将这些参数保存；\n            这种在使用from_pretrained方法加载模型时才不会出错；\n            \"\"\"\n            self.config.__dict__.update(model_args.__dict__)\n        self.num_labels = self.config.ner_num_labels\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(self.config.hidden_dropout)\n        self.lstm = nn.LSTM(self.config.hidden_size, # 输入的维度\n                            self.config.lstm_hidden_size, # 输出维度\n                            num_layers=self.config.lstm_layers, # 堆叠lstm的层数\n                            dropout=self.config.lstm_dropout,\n                            bidirectional=True, # 是否双向\n                            batch_first=True)\n        if self.config.use_lstm:\n            self.classifier = nn.Linear(self.config.lstm_hidden_size * 2, self.num_labels)\n        else:\n            self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n        self.init_weights()\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            labels=None,\n            pos=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = self.dropout(outputs[0])\n        if self.config.use_lstm:\n            sequence_output, _ = self.lstm(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            # 如果attention_mask不为空，则只计算attention_mask中为1部分的Loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits, # 该部分在评估时，会作为EvalPrediction对象的predictions进行返回\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:23.179559Z","iopub.execute_input":"2021-08-20T04:22:23.179983Z","iopub.status.idle":"2021-08-20T04:22:23.197458Z","shell.execute_reply.started":"2021-08-20T04:22:23.179945Z","shell.execute_reply":"2021-08-20T04:22:23.196619Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"测试一下模型是否符合预期","metadata":{}},{"cell_type":"code","source":"model_args = ModelArguments(use_lstm=True)\nmodel = BertForNER.from_pretrained(BERT_MODEL_NAME, model_args=model_args)\noutput = model(**batch)\nprint(type(output))\nprint(output.loss)\nprint(output.logits.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:23.198903Z","iopub.execute_input":"2021-08-20T04:22:23.199274Z","iopub.status.idle":"2021-08-20T04:22:49.687410Z","shell.execute_reply.started":"2021-08-20T04:22:23.199234Z","shell.execute_reply":"2021-08-20T04:22:49.686549Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba75683cced4e1483b1e38690460813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3698e9bf8f4c74a149f62fe0c91e39"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForNER: ['cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForNER were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'classifier.bias', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.bias_ih_l0_reverse', 'classifier.weight', 'lstm.bias_hh_l0_reverse', 'lstm.weight_hh_l0_reverse']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"<class 'transformers.modeling_outputs.TokenClassifierOutput'>\ntensor(3.4761, grad_fn=<NllLossBackward>)\ntorch.Size([2, 45, 34])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 六、模型训练","metadata":{}},{"cell_type":"code","source":"def run(model_args: ModelArguments, data_args: DataArguments, args: OurTrainingArguments):\n    # 设定训练参数\n    training_args = TrainingArguments(output_dir=args.checkpoint_dir,  # 训练中的checkpoint保存的位置\n                                      num_train_epochs=args.epoch,\n                                      do_eval=args.do_eval,  # 是否进行评估\n                                      evaluation_strategy=\"epoch\",  # 每个epoch结束后进行评估\n                                      per_device_train_batch_size=args.train_batch_size,\n                                      per_device_eval_batch_size=args.eval_batch_size,\n                                      load_best_model_at_end=True,  # 训练完成后加载最优模型\n                                      metric_for_best_model=\"f1\"  # 评估最优模型的指标，该指标是ner_metrics返回评估指标中的key\n                                      )\n     # 构建分词器\n    tokenizer = BertTokenizer.from_pretrained(args.bert_model_name)\n    \n    # 构建dataset\n    train_dataset = NERDataset(read_data(data_args.train_file), tokenizer=tokenizer)\n    eval_dataset = NERDataset(read_data(data_args.dev_file), tokenizer=tokenizer)\n    test_dataset = NERDataset(read_data(data_args.test_file), tokenizer=tokenizer)\n    \n    # 加载预训练模型\n    model = BertForNER.from_pretrained(args.bert_model_name, model_args=model_args)\n    # 初始化Trainer\n    trainer = Trainer(model=model,\n                      args=training_args,\n                      train_dataset=train_dataset,\n                      eval_dataset=eval_dataset,\n                      tokenizer=tokenizer,\n                      data_collator=collate_fn,\n                      compute_metrics=ner_metrics)\n    # 模型训练\n    trainer.train()\n    # 训练完成后，加载最优模型并进行评估\n    logger.info(trainer.evaluate(eval_dataset))\n    # 保存训练好的模型\n    trainer.save_model(args.best_dir)\n    \n    # 进行预测\n    logger.info(trainer.predict(test_dataset))\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:22:49.688768Z","iopub.execute_input":"2021-08-20T04:22:49.689132Z","iopub.status.idle":"2021-08-20T04:22:49.700567Z","shell.execute_reply.started":"2021-08-20T04:22:49.689104Z","shell.execute_reply":"2021-08-20T04:22:49.699771Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def main():\n    # 定义各类参数并训练模型\n    model_args = ModelArguments(use_lstm=True)\n    data_args = DataArguments()\n    training_args = OurTrainingArguments(bert_model_name=\"hfl/chinese-roberta-wwm-ext\",epoch=5,\n                                         train_batch_size=128, eval_batch_size=128)\n    run(model_args, data_args, training_args)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:32:27.423408Z","iopub.execute_input":"2021-08-20T04:32:27.423859Z","iopub.status.idle":"2021-08-20T04:32:27.433388Z","shell.execute_reply.started":"2021-08-20T04:32:27.423805Z","shell.execute_reply":"2021-08-20T04:32:27.432474Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T04:32:36.847228Z","iopub.execute_input":"2021-08-20T04:32:36.847613Z","iopub.status.idle":"2021-08-20T04:44:31.483030Z","shell.execute_reply.started":"2021-08-20T04:32:36.847582Z","shell.execute_reply":"2021-08-20T04:44:31.482168Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForNER: ['cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForNER were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0', 'classifier.bias', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.bias_ih_l0_reverse', 'classifier.weight', 'lstm.bias_hh_l0_reverse', 'lstm.weight_hh_l0_reverse']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [840/840 11:39, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.452647</td>\n      <td>0.364199</td>\n      <td>0.436083</td>\n      <td>0.390126</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.256539</td>\n      <td>0.713568</td>\n      <td>0.768139</td>\n      <td>0.701887</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.230775</td>\n      <td>0.774396</td>\n      <td>0.766954</td>\n      <td>0.786128</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.211439</td>\n      <td>0.792071</td>\n      <td>0.784034</td>\n      <td>0.802170</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.220745</td>\n      <td>0.786049</td>\n      <td>0.781213</td>\n      <td>0.793653</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.342100</td>\n      <td>0.238769</td>\n      <td>0.781475</td>\n      <td>0.780459</td>\n      <td>0.784691</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.342100</td>\n      <td>0.248653</td>\n      <td>0.783276</td>\n      <td>0.774141</td>\n      <td>0.793234</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.342100</td>\n      <td>0.254722</td>\n      <td>0.785502</td>\n      <td>0.777876</td>\n      <td>0.795465</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.342100</td>\n      <td>0.264056</td>\n      <td>0.781020</td>\n      <td>0.774723</td>\n      <td>0.789593</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.342100</td>\n      <td>0.263541</td>\n      <td>0.783215</td>\n      <td>0.774956</td>\n      <td>0.792627</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/11 00:06]\n    </div>\n    "},"metadata":{}}]}]}